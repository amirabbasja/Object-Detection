{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 16:09:38.483167: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-24 16:09:38.498777: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-24 16:09:38.503325: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-24 16:09:38.514525: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-24 16:09:39.429691: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1721824780.489476   59141 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1721824780.518001   59141 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1721824780.518185   59141 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1721824780.519057   59141 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1721824780.519190   59141 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1721824780.519306   59141 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1721824780.589225   59141 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1721824780.589398   59141 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1721824780.589541   59141 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-24 16:09:40.589634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4807 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from utils import iouUtils, calcIOU, lrScheduler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import sys, os\n",
    "from keras.regularizers import l2\n",
    "from YOLOv1_learning_Rate import customLearningRate\n",
    "from YOLOv1_Reshape_Layer import YOLOv1_LastLayer_Reshape\n",
    "from YOLOv1_Loss import YOLOv1_loss\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "here = os.path.dirname(\".\")\n",
    "sys.path.append(os.path.join(here, '..'))\n",
    "\n",
    "from dataHandler import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from utils import iouUtils, calcIOU\n",
    "\n",
    "\n",
    "def iouUtils(boxParams, gridRatio = tf.constant(7, tf.float32)):\n",
    "    \"\"\"\n",
    "    Given bounding box centers and its width and height, calculates top-left and bottom-right coordinates of the box.\n",
    "    Note that calculations in this function are done with teh assumption of w and h being a float number, between 0 and 1\n",
    "    with respect to the entire image's size. However, x and y of the bounding box's center are assumed to be a float \n",
    "    between 0 and 1, with respect to the upper-left point of the grid cell.\n",
    "\n",
    "    Args:\n",
    "        boxParams: tf.Tensor: A tensor with following information (Box center X, Box center Y, Box width, Box height) for all\n",
    "            boxes in a tensor.\n",
    "        gridRatio: int: The number of evenly distributed grid cells in each image axis. Use 7 for YOLOv1.\n",
    "    \n",
    "    Returns:\n",
    "        Two tensors, one indicating top-left pint of the bBox and, the other one denoting bottom-right edge.\n",
    "    \"\"\"\n",
    "    boxXY = boxParams[...,0:2]\n",
    "    halfWH = tf.divide(boxParams[...,2:], tf.constant([2.]))\n",
    "\n",
    "    # Top-left (X, Y) and bottom-right (X, Y)\n",
    "    return tf.subtract(boxXY, halfWH * gridRatio), tf.add(boxXY, halfWH * gridRatio)\n",
    "\n",
    "def calcIOU(predict_topLeft, predict_bottomRight, truth_topLeft, truth_bottomRight):\n",
    "    \"\"\"\n",
    "    Calculates intersection over union for two bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        predict_topLeft, predict_bottomRight: tf.Tensor: Top-left and bottom-right coordinates of the predicted box, acquired \n",
    "            by iouUtils.\n",
    "        truth_topLeft, truth_bottomRight: tf.Tensor: Top-left and bottom-right coordinates of the ground truth box, acquired \n",
    "            by iouUtils.\n",
    "    \n",
    "    Returns:\n",
    "        Intersection over union of two boxes\n",
    "    \"\"\"\n",
    "\n",
    "    intersectEdgeLeft = tf.maximum(predict_topLeft, truth_topLeft)\n",
    "    intersectEdgeRight = tf.minimum(predict_bottomRight, truth_bottomRight)\n",
    "    \n",
    "    intersectWH = tf.abs(tf.subtract(intersectEdgeLeft, intersectEdgeRight))\n",
    "    intersectArea = tf.reduce_prod(intersectWH, axis = -1)\n",
    "\n",
    "    # Get area of predicted and ground truth bounding boxes\n",
    "    predArea = tf.reduce_prod(tf.abs(tf.subtract(predict_topLeft, predict_bottomRight)), axis = -1)\n",
    "    truthArea = tf.reduce_prod(tf.abs(tf.subtract(truth_topLeft, truth_bottomRight)), axis = -1)\n",
    "\n",
    "    \n",
    "    # Return IOU\n",
    "    return tf.divide(intersectArea, predArea + truthArea - intersectArea)\n",
    "\n",
    "def lrScheduler(epoch, schedule, currentLR):\n",
    "    \"\"\"\n",
    "    Returns a learning rate value with respect to epoch number.\n",
    "\n",
    "    Args: \n",
    "        epoch: int: the current epoch number.\n",
    "        schedule: list: A list of tuples of epoch number and its respective learning rate value. \n",
    "            If the epoch number of the fitting process doesn't reach the specified epoch number,\n",
    "            the learning rate will remail unchanged. The entries have to be in order of epoch \n",
    "            numbers.\n",
    "        currentLR: float: The learning rate of the model before starting the most recent epoch.\n",
    "\n",
    "    Returns: float: learning rate.\n",
    "    \"\"\"\n",
    "    \n",
    "    newLR = currentLR\n",
    "\n",
    "    for entry in schedule:\n",
    "        if entry[0] == epoch:\n",
    "            newLR = float(entry[1])\n",
    "    \n",
    "    return newLR\n",
    "\n",
    "\n",
    "\n",
    "class customLearningRate(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Sets the learning rate of the fitting process with respect to the epoch number.\n",
    "\n",
    "    Args:\n",
    "        schedule: method: Using the epoch number, returns the suitable learning rate\n",
    "        LR_schedule: list: A list of tuples of epoch number and its respective learning rate value. \n",
    "            If the epoch number of the fitting process doesn't reach the specified epoch number,\n",
    "            the learning rate will remail unchanged. The entries have to be in order of epoch \n",
    "            numbers.\n",
    "    \"\"\"\n",
    "    def __init__(self, scheduleFCN, LR_schedule):\n",
    "        \"\"\"\n",
    "        Initialized the class\n",
    "\n",
    "        Args: \n",
    "            scheduleFCN: method: A method that returns new learning rate\n",
    "            LR_schedule: list: \n",
    "        \"\"\"\n",
    "        super(customLearningRate, self).__init__()\n",
    "        self.LR_schedule = LR_schedule\n",
    "        self.scheduleFCN = scheduleFCN\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        \"\"\"\n",
    "        Runs on the epoch start.\n",
    "\n",
    "        Args:\n",
    "            epoch: int: The current epoch number.\n",
    "        \"\"\"\n",
    "\n",
    "        # # Check to see of the model has defined a learning rate\n",
    "        # if hasattr(self.model.optimizer, \"lr\"):\n",
    "        #     raise Exception(\"custom learning rate generator: First define a learning rate for the model.\")\n",
    "        \n",
    "        # Get current learning rate\n",
    "        learningRate = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "\n",
    "        # Get the new learning rate\n",
    "        newLearningRate = self.scheduleFCN(epoch, self.LR_schedule, learningRate)\n",
    "\n",
    "        # Set the new learning rate as the model's learning rate\n",
    "        \n",
    "        self.model.optimizer.learning_rate.assign(newLearningRate)\n",
    "\n",
    "        # Notify the user\n",
    "        if learningRate != newLearningRate:\n",
    "            tf.print(f\"Updated the learning rate at epoch NO. {epoch}. New learning rate: {newLearningRate}\")\n",
    "\n",
    "\n",
    "class customLearningRate(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Sets the learning rate of the fitting process with respect to the epoch number.\n",
    "\n",
    "    Args:\n",
    "        schedule: method: Using the epoch number, returns the suitable learning rate\n",
    "        LR_schedule: list: A list of tuples of epoch number and its respective learning rate value. \n",
    "            If the epoch number of the fitting process doesn't reach the specified epoch number,\n",
    "            the learning rate will remail unchanged. The entries have to be in order of epoch \n",
    "            numbers.\n",
    "    \"\"\"\n",
    "    def __init__(self, scheduleFCN, LR_schedule):\n",
    "        \"\"\"\n",
    "        Initialized the class\n",
    "\n",
    "        Args: \n",
    "            scheduleFCN: method: A method that returns new learning rate\n",
    "            LR_schedule: list: \n",
    "        \"\"\"\n",
    "        super(customLearningRate, self).__init__()\n",
    "        self.LR_schedule = LR_schedule\n",
    "        self.scheduleFCN = scheduleFCN\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        \"\"\"\n",
    "        Runs on the epoch start.\n",
    "\n",
    "        Args:\n",
    "            epoch: int: The current epoch number.\n",
    "        \"\"\"\n",
    "\n",
    "        # # Check to see of the model has defined a learning rate\n",
    "        # if hasattr(self.model.optimizer, \"lr\"):\n",
    "        #     raise Exception(\"custom learning rate generator: First define a learning rate for the model.\")\n",
    "        \n",
    "        # Get current learning rate\n",
    "        learningRate = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "\n",
    "        # Get the new learning rate\n",
    "        newLearningRate = self.scheduleFCN(epoch, self.LR_schedule, learningRate)\n",
    "\n",
    "        # Set the new learning rate as the model's learning rate\n",
    "        \n",
    "        self.model.optimizer.learning_rate.assign(newLearningRate)\n",
    "\n",
    "        # Notify the user\n",
    "        if learningRate != newLearningRate:\n",
    "            tf.print(f\"Updated the learning rate at epoch NO. {epoch}. New learning rate: {newLearningRate}\")\n",
    "\n",
    "\n",
    "def YOLOv1_loss(yTrue, yPred):\n",
    "    \"\"\"\n",
    "    Runs in the even of loss function calculations\n",
    "    \n",
    "    Args:\n",
    "        yTrue, yPred: tf.Tensor: The ground truth value and the predicted value, respectively\n",
    "\n",
    "    Returns:\n",
    "        The calculated loss.\n",
    "    \"\"\"\n",
    "    lambdaNoObj = tf.constant(.5)\n",
    "    lambdaCoord = tf.constant(5.)\n",
    "\n",
    "    # Split the predictions and ground truth vectors to coordinates, confidence and class matrices\n",
    "    # 1. Ground truth \n",
    "    idx1, idx2 = 1, 1 + 1\n",
    "    targetClass = yTrue[...,:idx1]\n",
    "    targetConf = yTrue[...,idx1:idx2]\n",
    "    targetCoords = yTrue[...,idx2:]\n",
    "\n",
    "    # 2. Prediction\n",
    "    idx1, idx2 = 1, 1 + 2\n",
    "    predClass = yPred[...,:idx1]\n",
    "    predConf = yPred[...,idx1:idx2]\n",
    "    predCoords = yPred[...,idx2:]\n",
    "\n",
    "    # Get the best bounding boxes by calculating the IOUs\n",
    "    # Note: To to do this process for the confidence scores as well, we concat each box's confidence\n",
    "    # score to its bounding box coordinates and analyze them as a whole.\n",
    "    predBox1 = tf.concat([tf.expand_dims(predConf[...,0],-1),predCoords[...,:4]], axis = -1)\n",
    "    predBox2 = tf.concat([tf.expand_dims(predConf[...,1],-1),predCoords[...,4:]], axis = -1)\n",
    "\n",
    "    # Get the corners of bounding boxes to calculate IOUs\n",
    "    # Note, iouUtils is not coded to accept confidence scores. So we only pass the coordinates into \n",
    "    # it. \n",
    "    p1_left, p1_right = iouUtils(predBox1[...,1:]) \n",
    "    p2_left, p2_right = iouUtils(predBox2[...,1:])\n",
    "    t_left, t_right = iouUtils(targetCoords) \n",
    "\n",
    "    # Calculate IOUs for first and second predicted bounding box\n",
    "    p1_IOU = calcIOU(p1_left, p1_right, t_left, t_right)\n",
    "    p2_IOU = calcIOU(p2_left, p2_right, t_left, t_right)\n",
    "\n",
    "    # Get the cells that have objects\n",
    "    maskObj = tf.cast(0 < targetConf, tf.float32)\n",
    "    maskNoObj = tf.cast(0 == targetConf, tf.float32)\n",
    "    \n",
    "    mask_p1Bigger = tf.expand_dims(tf.cast(p2_IOU < p1_IOU, tf.float32),-1)\n",
    "    mask_p2Bigger = tf.expand_dims(tf.cast(p1_IOU <= p2_IOU, tf.float32),-1)\n",
    "\n",
    "    # Getting the responsible bounding box for loss calculation. Output is of shape [...,5]\n",
    "    # And the first element is the confidence score of that box.\n",
    "    respBox = maskObj*(mask_p1Bigger * predBox1 + mask_p2Bigger * predBox2)\n",
    "\n",
    "    # Calculating the losses\n",
    "    # 1. Classification loss\n",
    "    classificationLoss =  tf.math.reduce_sum(tf.math.square(maskObj * tf.subtract(targetClass, predClass)))\n",
    "\n",
    "    # 2. Confidence loss\n",
    "    # Bear in mind, for the boxes with no objects, we account for the confidence loss as well. \n",
    "    # To penalize the network for high confidence scores of the cells containing no objects. The \n",
    "    # cells that have no objects, have a confidence score of 0 in the target ground truth matrix.\n",
    "    # Thus, the loss is calculated as follows: SUM_All_Cells_No_OBJ((C1-0)^2 + (C2-0)^2)\n",
    "    confidenceLossObj = tf.math.reduce_sum(tf.math.square(maskObj * tf.subtract(targetConf, tf.expand_dims(respBox[...,0],-1))))\n",
    "    confidenceLossNoObj =  lambdaNoObj * tf.reduce_sum(maskNoObj * tf.reduce_sum(tf.square(predConf), axis = -1, keepdims = True))\n",
    "    \n",
    "    # 3. Localization loss\n",
    "    # Bear in mind that respBox is of the shape (...,5) and targetCoords dimension is (...,4) \n",
    "    xyLoss = (tf.reduce_sum(tf.square(tf.subtract(respBox[...,1:3], targetCoords[...,0:2])),-1,True))\n",
    "    whLoss = (tf.reduce_sum(tf.square(tf.subtract(tf.sqrt(respBox[...,1:3]), tf.sqrt(targetCoords[...,0:2]))),-1,True))\n",
    "    localizationLoss = lambdaCoord * (xyLoss + whLoss) \n",
    "\n",
    "    # Sum all the tree types of the errors\n",
    "    return classificationLoss + confidenceLossNoObj + confidenceLossObj + localizationLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLOv1 structure\n",
    "YOLOv1_inputShape = (448,448,3) # Shape of the input image \n",
    "classNo = 1 # Number of classes we are trying to detect\n",
    "input = tf.keras.layers.Input(shape=YOLOv1_inputShape)\n",
    "leakyReLu = tf.keras.layers.LeakyReLU(negative_slope = .1)\n",
    "\n",
    "\n",
    "# The backbone, Acts ads a feature extractor\n",
    "# L1\n",
    "x = tf.keras.layers.Conv2D(filters = 64, kernel_size=7, strides = 2, padding = \"same\", activation= leakyReLu, kernel_regularizer=l2(1e-5))(input)\n",
    "x = tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding = \"same\")(x)\n",
    "\n",
    "# L2\n",
    "x = tf.keras.layers.Conv2D(filters = 192, kernel_size=3, strides = 1, padding = \"same\", activation= leakyReLu, kernel_regularizer=l2(1e-5))(x)\n",
    "x = tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding = \"same\")(x)\n",
    "\n",
    "# L3\n",
    "x = tf.keras.layers.Conv2D(filters = 128, kernel_size=1, strides = 1, padding = \"same\", activation= leakyReLu, kernel_regularizer=l2(1e-5))(x)\n",
    "x = tf.keras.layers.Conv2D(filters = 256, kernel_size=3, strides = 1, padding = \"same\", activation= leakyReLu, kernel_regularizer=l2(1e-5))(x)\n",
    "x = tf.keras.layers.Conv2D(filters = 256, kernel_size=1, strides = 1, padding = \"same\", activation= leakyReLu, kernel_regularizer=l2(1e-5))(x)\n",
    "x = tf.keras.layers.Conv2D(filters = 512, kernel_size=3, strides = 1, padding = \"same\", activation= leakyReLu, kernel_regularizer=l2(1e-5))(x)\n",
    "x = tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding = \"same\")(x)\n",
    "\n",
    "# L4\n",
    "for _ in range(4):\n",
    "    x = tf.keras.layers.Conv2D(filters = 256, kernel_size=1, strides = 1, padding = \"same\", activation= leakyReLu, kernel_regularizer=l2(1e-5))(x)\n",
    "    x = tf.keras.layers.Conv2D(filters = 512, kernel_size=3, strides = 1, padding = \"same\", activation= leakyReLu, kernel_regularizer=l2(1e-5))(x)\n",
    "x = tf.keras.layers.Conv2D(filters = 512, kernel_size=1, strides = 1, padding = \"same\", activation= leakyReLu, kernel_regularizer=l2(1e-5))(x)\n",
    "x = tf.keras.layers.Conv2D(filters = 1024, kernel_size=3, strides = 1, padding = \"same\", activation= leakyReLu, kernel_regularizer=l2(1e-5))(x)\n",
    "x = tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding = \"same\")(x)\n",
    "\n",
    "# L5\n",
    "x = tf.keras.layers.Conv2D(filters = 512, kernel_size=1, strides = 1, padding = \"same\", activation= leakyReLu, kernel_regularizer=l2(1e-5))(x)\n",
    "x = tf.keras.layers.Conv2D(filters = 1024, kernel_size=3, strides = 1, padding = \"same\", activation= leakyReLu, kernel_regularizer=l2(1e-5))(x)\n",
    "x = tf.keras.layers.Conv2D(filters = 512, kernel_size=1, strides = 1, padding = \"same\", activation= leakyReLu, kernel_regularizer=l2(1e-5))(x)\n",
    "x = tf.keras.layers.Conv2D(filters = 1024, kernel_size=3, strides = 1, padding = \"same\", activation= leakyReLu, kernel_regularizer=l2(1e-5))(x)\n",
    "x = tf.keras.layers.Conv2D(filters = 1024, kernel_size=3, strides = 1, padding = \"same\", activation= leakyReLu, kernel_regularizer=l2(1e-5))(x)\n",
    "x = tf.keras.layers.Conv2D(filters = 1024, kernel_size=3, strides = 2, padding = \"same\", activation= leakyReLu, kernel_regularizer=l2(1e-5))(x)\n",
    "\n",
    "# L6\n",
    "x = tf.keras.layers.Conv2D(filters = 1024, kernel_size=3, strides = 1, padding = \"same\", activation= leakyReLu, kernel_regularizer=l2(1e-5))(x)\n",
    "x = tf.keras.layers.Conv2D(filters = 1024, kernel_size=3, strides = 1, padding = \"same\", activation= leakyReLu, kernel_regularizer=l2(1e-5))(x)\n",
    "\n",
    "# Neck\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(4096)(x)\n",
    "x = tf.keras.layers.Dense(7*7*(5*2+classNo), activation=\"sigmoid\")(x)\n",
    "x = tf.keras.layers.Dropout(.5)(x) # Dropout layer for avoiding overfitting\n",
    "x = YOLOv1_LastLayer_Reshape((7,7,5*2+classNo))(x)\n",
    "model = tf.keras.Model(inputs = input, outputs = x, name = \"YOLOv1\")\n",
    "\n",
    "model.compile(loss = YOLOv1_loss ,optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 982, testing data: 983\n",
      "Updated the learning rate at epoch NO. 0. New learning rate: 0.01\n",
      "Epoch 1/135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1721824787.362266   59215 service.cc:146] XLA service 0x7f1efc003b80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1721824787.362300   59215 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-07-24 16:09:47.607058: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-07-24 16:09:48.220445: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n",
      "2024-07-24 16:09:50.159559: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.30GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-24 16:09:50.159612: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.30GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-24 16:09:50.159624: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.27GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-24 16:09:50.235383: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.27GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-24 16:09:50.235415: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 9.14GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-24 16:09:50.235426: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 9.14GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-24 16:09:50.235436: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 17.10GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-24 16:09:50.287965: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.30GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-24 16:09:50.287998: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.30GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-24 16:09:50.288009: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.27GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "E0000 00:00:1721824791.439256   59215 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1721824791.581252   59215 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2024-07-24 16:09:54.798713: W external/local_xla/xla/service/hlo_rematerialization.cc:3005] Can't reduce memory use below 1.56GiB (1672484982 bytes) by rematerialization; only reduced to 3.36GiB (3606565636 bytes), down from 3.87GiB (4160077193 bytes) originally\n",
      "2024-07-24 16:09:55.120574: W external/local_xla/xla/service/gpu/nvptx_compiler.cc:762] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.3.107). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  1/982\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:23:43\u001b[0m 16s/step - loss: 16.5563"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 16:09:56.989745: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_12', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "I0000 00:00:1721824797.023233   59215 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m982/982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: nan\n",
      "Epoch 1: val_loss did not improve from inf\n",
      "\u001b[1m982/982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 114ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 16:11:49.180614: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2024-07-24 16:11:49.180647: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_2]]\n",
      "/home/abbas/miniconda3/envs/ml/lib/python3.11/contextlib.py:155: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n",
      "2024-07-24 16:11:49.199337: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_2]]\n",
      "2024-07-24 16:11:49.199380: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 6237602946751916786\n",
      "2024-07-24 16:11:49.199396: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 14912214771012437356\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 28\u001b[0m\n\u001b[1;32m     24\u001b[0m testingBatchGenerator \u001b[38;5;241m=\u001b[39m dataGenerator_YOLOv1(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/images/test\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_size, (\u001b[38;5;241m448\u001b[39m,\u001b[38;5;241m448\u001b[39m), dfTrain, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainingBatchGenerator\u001b[38;5;241m.\u001b[39mindexes\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, testing data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtestingBatchGenerator\u001b[38;5;241m.\u001b[39mindexes\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainingBatchGenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrainingBatchGenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindexes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m135\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtestingBatchGenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtestingBatchGenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindexes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcustomLearningRate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlrScheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLR_schedule\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchkPoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:354\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_epoch_iterator \u001b[38;5;241m=\u001b[39m TFEpochIterator(\n\u001b[1;32m    334\u001b[0m             x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[1;32m    335\u001b[0m             y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    341\u001b[0m             shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    342\u001b[0m         )\n\u001b[1;32m    343\u001b[0m     val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(\n\u001b[1;32m    344\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[1;32m    345\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    351\u001b[0m         _use_cached_eval_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    352\u001b[0m     )\n\u001b[1;32m    353\u001b[0m     val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 354\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m \u001b[43mval_logs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m()\n\u001b[1;32m    355\u001b[0m     }\n\u001b[1;32m    356\u001b[0m     epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n\u001b[1;32m    358\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_epoch_end(epoch, epoch_logs)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "# See if the directory to save the checkpoints exists\n",
    "if not os.path.isdir(f\"{os.getcwd()}/model_data\"):\n",
    "    os.mkdir(f\"{os.getcwd()}/model_data\")\n",
    "\n",
    "# Instantiate the checkpoint object\n",
    "chkPoint = ModelCheckpoint(filepath='./model_data/model_{epoch:02d}-{val_loss:.2f}.keras',\n",
    "                                    save_best_only=True,\n",
    "                                    monitor='val_loss',\n",
    "                                    mode='min',\n",
    "                                    verbose=1\n",
    "                            )\n",
    "\n",
    "batch_size = 1\n",
    "LR_schedule = [\n",
    "    (0, 0.01),\n",
    "    (75, 0.001),\n",
    "    (105, 0.0001),\n",
    "]\n",
    "\n",
    "dfTrain = annotationsToDataframe(f\"../data/labels/train\", \"txt\")\n",
    "trainingBatchGenerator = dataGenerator_YOLOv1(f\"../data/images/train\", batch_size, (448,448), dfTrain, 1, True)\n",
    "\n",
    "dfTest = annotationsToDataframe(f\"../data/labels/test\", \"txt\")\n",
    "testingBatchGenerator = dataGenerator_YOLOv1(f\"../data/images/test\", batch_size, (448,448), dfTrain, 1, True)\n",
    "\n",
    "print(f\"Training data: {trainingBatchGenerator.indexes.shape[0]}, testing data: {testingBatchGenerator.indexes.shape[0]}\")\n",
    "\n",
    "model.fit(x=trainingBatchGenerator,\n",
    "        steps_per_epoch = int(trainingBatchGenerator.indexes.shape[0] // batch_size),\n",
    "        epochs = 135,\n",
    "        verbose = 1,\n",
    "        validation_data = testingBatchGenerator,\n",
    "        validation_steps = int(testingBatchGenerator.indexes.shape[0] // batch_size),\n",
    "        callbacks = [\n",
    "            customLearningRate(lrScheduler, LR_schedule),\n",
    "            chkPoint,\n",
    "        ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
