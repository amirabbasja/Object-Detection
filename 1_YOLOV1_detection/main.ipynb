{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 19:30:13.005110: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-30 19:30:13.019000: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-30 19:30:13.022676: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-30 19:30:13.032001: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-30 19:30:13.862765: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722355214.794416   14212 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1722355214.821316   14212 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1722355214.821461   14212 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1722355214.822502   14212 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1722355214.822698   14212 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1722355214.822820   14212 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1722355214.879056   14212 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1722355214.879213   14212 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1722355214.879320   14212 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-30 19:30:14.879405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4767 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from utils import iouUtils, calcIOU, lrScheduler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import sys, os\n",
    "from keras.regularizers import l2\n",
    "from YOLOv1_learning_Rate import customLearningRate\n",
    "from YOLOv1_Reshape_Layer import YOLOv1_LastLayer_Reshape\n",
    "from YOLOv1_Loss import YOLOv1_loss\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "here = os.path.dirname(\".\")\n",
    "sys.path.append(os.path.join(here, '..'))\n",
    "\n",
    "from dataHandler import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from utils import iouUtils, calcIOU, customSQRT2\n",
    "\n",
    "\n",
    "def iouUtils(boxParams, gridRatio = tf.constant(7, tf.float32)):\n",
    "    \"\"\"\n",
    "    Given bounding box centers and its width and height, calculates top-left and bottom-right coordinates of the box.\n",
    "    Note that calculations in this function are done with teh assumption of w and h being a float number, between 0 and 1\n",
    "    with respect to the entire image's size. However, x and y of the bounding box's center are assumed to be a float \n",
    "    between 0 and 1, with respect to the upper-left point of the grid cell.\n",
    "\n",
    "    Args:\n",
    "        boxParams: tf.Tensor: A tensor with following information (Box center X, Box center Y, Box width, Box height) for all\n",
    "            boxes in a tensor.\n",
    "        gridRatio: int: The number of evenly distributed grid cells in each image axis. Use 7 for YOLOv1.\n",
    "    \n",
    "    Returns:\n",
    "        Two tensors, one indicating top-left pint of the bBox and, the other one denoting bottom-right edge.\n",
    "    \"\"\"\n",
    "    boxXY = boxParams[...,0:2]\n",
    "    halfWH = tf.divide(boxParams[...,2:], tf.constant([2.]))\n",
    "\n",
    "    # Top-left (X, Y) and bottom-right (X, Y)\n",
    "    return tf.subtract(boxXY, halfWH * gridRatio), tf.add(boxXY, halfWH * gridRatio)\n",
    "\n",
    "def calcIOU(predict_topLeft, predict_bottomRight, truth_topLeft, truth_bottomRight):\n",
    "    \"\"\"\n",
    "    Calculates intersection over union for two bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        predict_topLeft, predict_bottomRight: tf.Tensor: Top-left and bottom-right coordinates of the predicted box, acquired \n",
    "            by iouUtils.\n",
    "        truth_topLeft, truth_bottomRight: tf.Tensor: Top-left and bottom-right coordinates of the ground truth box, acquired \n",
    "            by iouUtils.\n",
    "    \n",
    "    Returns:\n",
    "        Intersection over union of two boxes\n",
    "    \"\"\"\n",
    "\n",
    "    intersectEdgeLeft = tf.maximum(predict_topLeft, truth_topLeft)\n",
    "    intersectEdgeRight = tf.minimum(predict_bottomRight, truth_bottomRight)\n",
    "    \n",
    "    intersectWH = tf.abs(tf.subtract(intersectEdgeLeft, intersectEdgeRight))\n",
    "    intersectArea = tf.reduce_prod(intersectWH, axis = -1)\n",
    "\n",
    "    # Get area of predicted and ground truth bounding boxes\n",
    "    predArea = tf.reduce_prod(tf.abs(tf.subtract(predict_topLeft, predict_bottomRight)), axis = -1)\n",
    "    truthArea = tf.reduce_prod(tf.abs(tf.subtract(truth_topLeft, truth_bottomRight)), axis = -1)\n",
    "\n",
    "    \n",
    "    # Return IOU\n",
    "    return tf.divide(intersectArea, predArea + truthArea - intersectArea)\n",
    "\n",
    "def lrScheduler(epoch, schedule, currentLR):\n",
    "    \"\"\"\n",
    "    Returns a learning rate value with respect to epoch number.\n",
    "\n",
    "    Args: \n",
    "        epoch: int: the current epoch number.\n",
    "        schedule: list: A list of tuples of epoch number and its respective learning rate value. \n",
    "            If the epoch number of the fitting process doesn't reach the specified epoch number,\n",
    "            the learning rate will remail unchanged. The entries have to be in order of epoch \n",
    "            numbers.\n",
    "        currentLR: float: The learning rate of the model before starting the most recent epoch.\n",
    "\n",
    "    Returns: float: learning rate.\n",
    "    \"\"\"\n",
    "    \n",
    "    newLR = currentLR\n",
    "\n",
    "    for entry in schedule:\n",
    "        if entry[0] == epoch:\n",
    "            newLR = float(entry[1])\n",
    "    \n",
    "    return newLR\n",
    "\n",
    "\n",
    "\n",
    "class customLearningRate(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Sets the learning rate of the fitting process with respect to the epoch number.\n",
    "\n",
    "    Args:\n",
    "        schedule: method: Using the epoch number, returns the suitable learning rate\n",
    "        LR_schedule: list: A list of tuples of epoch number and its respective learning rate value. \n",
    "            If the epoch number of the fitting process doesn't reach the specified epoch number,\n",
    "            the learning rate will remail unchanged. The entries have to be in order of epoch \n",
    "            numbers.\n",
    "    \"\"\"\n",
    "    def __init__(self, scheduleFCN, LR_schedule):\n",
    "        \"\"\"\n",
    "        Initialized the class\n",
    "\n",
    "        Args: \n",
    "            scheduleFCN: method: A method that returns new learning rate\n",
    "            LR_schedule: list: \n",
    "        \"\"\"\n",
    "        super(customLearningRate, self).__init__()\n",
    "        self.LR_schedule = LR_schedule\n",
    "        self.scheduleFCN = scheduleFCN\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        \"\"\"\n",
    "        Runs on the epoch start.\n",
    "\n",
    "        Args:\n",
    "            epoch: int: The current epoch number.\n",
    "        \"\"\"\n",
    "\n",
    "        # # Check to see of the model has defined a learning rate\n",
    "        # if hasattr(self.model.optimizer, \"lr\"):\n",
    "        #     raise Exception(\"custom learning rate generator: First define a learning rate for the model.\")\n",
    "        \n",
    "        # Get current learning rate\n",
    "        learningRate = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "\n",
    "        # Get the new learning rate\n",
    "        newLearningRate = self.scheduleFCN(epoch, self.LR_schedule, learningRate)\n",
    "\n",
    "        # Set the new learning rate as the model's learning rate\n",
    "        \n",
    "        self.model.optimizer.learning_rate.assign(newLearningRate)\n",
    "\n",
    "        # Notify the user\n",
    "        if learningRate != newLearningRate:\n",
    "            tf.print(f\"Updated the learning rate at epoch NO. {epoch}. New learning rate: {newLearningRate}\")\n",
    "\n",
    "\n",
    "class customLearningRate(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Sets the learning rate of the fitting process with respect to the epoch number.\n",
    "\n",
    "    Args:\n",
    "        schedule: method: Using the epoch number, returns the suitable learning rate\n",
    "        LR_schedule: list: A list of tuples of epoch number and its respective learning rate value. \n",
    "            If the epoch number of the fitting process doesn't reach the specified epoch number,\n",
    "            the learning rate will remail unchanged. The entries have to be in order of epoch \n",
    "            numbers.\n",
    "    \"\"\"\n",
    "    def __init__(self, scheduleFCN, LR_schedule):\n",
    "        \"\"\"\n",
    "        Initialized the class\n",
    "\n",
    "        Args: \n",
    "            scheduleFCN: method: A method that returns new learning rate\n",
    "            LR_schedule: list: \n",
    "        \"\"\"\n",
    "        super(customLearningRate, self).__init__()\n",
    "        self.LR_schedule = LR_schedule\n",
    "        self.scheduleFCN = scheduleFCN\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        \"\"\"\n",
    "        Runs on the epoch start.\n",
    "\n",
    "        Args:\n",
    "            epoch: int: The current epoch number.\n",
    "        \"\"\"\n",
    "\n",
    "        # # Check to see of the model has defined a learning rate\n",
    "        # if hasattr(self.model.optimizer, \"lr\"):\n",
    "        #     raise Exception(\"custom learning rate generator: First define a learning rate for the model.\")\n",
    "        \n",
    "        # Get current learning rate\n",
    "        learningRate = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "\n",
    "        # Get the new learning rate\n",
    "        newLearningRate = self.scheduleFCN(epoch, self.LR_schedule, learningRate)\n",
    "\n",
    "        # Set the new learning rate as the model's learning rate\n",
    "        \n",
    "        self.model.optimizer.learning_rate.assign(newLearningRate)\n",
    "\n",
    "        # Notify the user\n",
    "        if learningRate != newLearningRate:\n",
    "            tf.print(f\"Updated the learning rate at epoch NO. {epoch}. New learning rate: {newLearningRate}\")\n",
    "\n",
    "\n",
    "def YOLOv1_loss(yTrue, yPred):\n",
    "    \"\"\"\n",
    "    Runs in the even of loss function calculations\n",
    "    \n",
    "    Args:\n",
    "        yTrue, yPred: tf.Tensor: The ground truth value and the predicted value, respectively\n",
    "\n",
    "    Returns:\n",
    "        The calculated loss.\n",
    "    \"\"\"\n",
    "    lambdaNoObj = tf.constant(.5)\n",
    "    lambdaCoord = tf.constant(5.)\n",
    "\n",
    "    # Split the predictions and ground truth vectors to coordinates, confidence and class matrices\n",
    "    # 1. Ground truth \n",
    "    idx1, idx2 = 1, 1 + 1\n",
    "    targetClass = yTrue[...,:idx1]\n",
    "    targetConf = yTrue[...,idx1:idx2]\n",
    "    targetCoords = yTrue[...,idx2:]\n",
    "\n",
    "    # 2. Prediction\n",
    "    idx1, idx2 = 1, 1 + 2\n",
    "    predClass = yPred[...,:idx1]\n",
    "    predConf = yPred[...,idx1:idx2]\n",
    "    predCoords = yPred[...,idx2:]\n",
    "\n",
    "    # Get the best bounding boxes by calculating the IOUs\n",
    "    # Note: To to do this process for the confidence scores as well, we concat each box's confidence\n",
    "    # score to its bounding box coordinates and analyze them as a whole.\n",
    "    predBox1 = tf.concat([tf.expand_dims(predConf[...,0],-1),predCoords[...,:4]], axis = -1)\n",
    "    predBox2 = tf.concat([tf.expand_dims(predConf[...,1],-1),predCoords[...,4:]], axis = -1)\n",
    "\n",
    "    # Get the corners of bounding boxes to calculate IOUs\n",
    "    # Note, iouUtils is not coded to accept confidence scores. So we only pass the coordinates into \n",
    "    # it. \n",
    "    p1_left, p1_right = iouUtils(predBox1[...,1:]) \n",
    "    p2_left, p2_right = iouUtils(predBox2[...,1:])\n",
    "    t_left, t_right = iouUtils(targetCoords) \n",
    "\n",
    "    # Calculate IOUs for first and second predicted bounding box\n",
    "    p1_IOU = calcIOU(p1_left, p1_right, t_left, t_right)\n",
    "    p2_IOU = calcIOU(p2_left, p2_right, t_left, t_right)\n",
    "\n",
    "    # Get the cells that have objects\n",
    "    maskObj = tf.cast(0 < targetConf, tf.float32)\n",
    "    maskNoObj = tf.cast(0 == targetConf, tf.float32)\n",
    "    \n",
    "    mask_p1Bigger = tf.expand_dims(tf.cast(p2_IOU < p1_IOU, tf.float32),-1)\n",
    "    mask_p2Bigger = tf.expand_dims(tf.cast(p1_IOU <= p2_IOU, tf.float32),-1)\n",
    "\n",
    "    # Getting the responsible bounding box for loss calculation. Output is of shape [...,5]\n",
    "    # And the first element is the confidence score of that box.\n",
    "    respBox = maskObj*(mask_p1Bigger * predBox1 + mask_p2Bigger * predBox2)\n",
    "\n",
    "    # Calculating the losses\n",
    "    # 1. Classification loss\n",
    "    classificationLoss =  tf.math.reduce_sum(tf.math.square(maskObj * tf.subtract(targetClass, predClass)))\n",
    "\n",
    "    # 2. Confidence loss\n",
    "    # Bear in mind, for the boxes with no objects, we account for the confidence loss as well. \n",
    "    # To penalize the network for high confidence scores of the cells containing no objects. The \n",
    "    # cells that have no objects, have a confidence score of 0 in the target ground truth matrix.\n",
    "    # Thus, the loss is calculated as follows: SUM_All_Cells_No_OBJ((C1-0)^2 + (C2-0)^2)\n",
    "    confidenceLossObj = tf.math.reduce_sum(tf.math.square(maskObj * tf.subtract(targetConf, tf.expand_dims(respBox[...,0],-1))))\n",
    "    confidenceLossNoObj =  lambdaNoObj * tf.reduce_sum(maskNoObj * tf.reduce_sum(tf.square(predConf), axis = -1, keepdims = True))\n",
    "    \n",
    "    # 3. Localization loss\n",
    "    # Bear in mind that respBox is of the shape (...,5) and targetCoords dimension is (...,4) \n",
    "    xyLoss = (tf.reduce_sum(tf.square(tf.subtract(respBox[...,1:3], targetCoords[...,0:2])),-1,True))\n",
    "    whLoss = (tf.reduce_sum(tf.square(tf.subtract(customSQRT2(respBox[...,1:3]), customSQRT2(targetCoords[...,0:2]))),-1,True))\n",
    "    localizationLoss = lambdaCoord * (xyLoss + whLoss) \n",
    "\n",
    "    # Sum all the tree types of the errors\n",
    "    return classificationLoss + confidenceLossNoObj + confidenceLossObj + localizationLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLOv1 structure\n",
    "YOLOv1_inputShape = (448,448,3) # Shape of the input image \n",
    "classNo = 1 # Number of classes we are trying to detect\n",
    "input = tf.keras.layers.Input(shape=YOLOv1_inputShape)\n",
    "leakyReLu = tf.keras.layers.LeakyReLU(negative_slope = .1)\n",
    "\n",
    "\n",
    "# The backbone, Acts ads a feature extractor\n",
    "# L1\n",
    "x = tf.keras.layers.Conv2D(filters = 64, kernel_size=7, strides = 2, padding = \"same\", activation= leakyReLu, kernel_regularizer=l2(1e-5))(input)\n",
    "x = tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding = \"same\")(x)\n",
    "\n",
    "# L2\n",
    "x = tf.keras.layers.Conv2D(filters = 192, kernel_size=3, strides = 1, padding = \"same\", activation= leakyReLu, kernel_regularizer=l2(1e-5))(x)\n",
    "x = tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding = \"same\")(x)\n",
    "\n",
    "# L3\n",
    "x = tf.keras.layers.Conv2D(filters = 128, kernel_size=1, strides = 1, padding = \"same\", activation= leakyReLu, kernel_regularizer=l2(1e-5))(x)\n",
    "x = tf.keras.layers.Conv2D(filters = 256, kernel_size=3, strides = 1, padding = \"same\", activation= leakyReLu, kernel_regularizer=l2(1e-5))(x)\n",
    "x = tf.keras.layers.Conv2D(filters = 256, kernel_size=1, strides = 1, padding = \"same\", activation= leakyReLu, kernel_regularizer=l2(1e-5))(x)\n",
    "x = tf.keras.layers.Conv2D(filters = 512, kernel_size=3, strides = 1, padding = \"same\", activation= leakyReLu, kernel_regularizer=l2(1e-5))(x)\n",
    "x = tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding = \"same\")(x)\n",
    "\n",
    "# L4\n",
    "for _ in range(4):\n",
    "    x = tf.keras.layers.Conv2D(filters = 256, kernel_size=1, strides = 1, padding = \"same\", activation= leakyReLu, kernel_regularizer=l2(1e-5))(x)\n",
    "    x = tf.keras.layers.Conv2D(filters = 512, kernel_size=3, strides = 1, padding = \"same\", activation= leakyReLu, kernel_regularizer=l2(1e-5))(x)\n",
    "x = tf.keras.layers.Conv2D(filters = 512, kernel_size=1, strides = 1, padding = \"same\", activation= leakyReLu, kernel_regularizer=l2(1e-5))(x)\n",
    "x = tf.keras.layers.Conv2D(filters = 1024, kernel_size=3, strides = 1, padding = \"same\", activation= leakyReLu, kernel_regularizer=l2(1e-5))(x)\n",
    "x = tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding = \"same\")(x)\n",
    "\n",
    "# L5\n",
    "x = tf.keras.layers.Conv2D(filters = 512, kernel_size=1, strides = 1, padding = \"same\", activation= leakyReLu, kernel_regularizer=l2(1e-5))(x)\n",
    "x = tf.keras.layers.Conv2D(filters = 1024, kernel_size=3, strides = 1, padding = \"same\", activation= leakyReLu, kernel_regularizer=l2(1e-5))(x)\n",
    "x = tf.keras.layers.Conv2D(filters = 512, kernel_size=1, strides = 1, padding = \"same\", activation= leakyReLu, kernel_regularizer=l2(1e-5))(x)\n",
    "x = tf.keras.layers.Conv2D(filters = 1024, kernel_size=3, strides = 1, padding = \"same\", activation= leakyReLu, kernel_regularizer=l2(1e-5))(x)\n",
    "x = tf.keras.layers.Conv2D(filters = 1024, kernel_size=3, strides = 1, padding = \"same\", activation= leakyReLu, kernel_regularizer=l2(1e-5))(x)\n",
    "x = tf.keras.layers.Conv2D(filters = 1024, kernel_size=3, strides = 2, padding = \"same\", activation= leakyReLu, kernel_regularizer=l2(1e-5))(x)\n",
    "\n",
    "# L6\n",
    "x = tf.keras.layers.Conv2D(filters = 1024, kernel_size=3, strides = 1, padding = \"same\", activation= leakyReLu, kernel_regularizer=l2(1e-5))(x)\n",
    "x = tf.keras.layers.Conv2D(filters = 1024, kernel_size=3, strides = 1, padding = \"same\", activation= leakyReLu, kernel_regularizer=l2(1e-5))(x)\n",
    "\n",
    "# Neck\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(4096)(x)\n",
    "x = tf.keras.layers.Dense(7*7*(5*2+classNo), activation=\"sigmoid\")(x)\n",
    "x = tf.keras.layers.Dropout(.5)(x) # Dropout layer for avoiding overfitting\n",
    "x = YOLOv1_LastLayer_Reshape((7,7,5*2+classNo))(x)\n",
    "model = tf.keras.Model(inputs = input, outputs = x, name = \"YOLOv1\")\n",
    "\n",
    "model.compile(loss = YOLOv1_loss ,optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 982, testing data: 983\n",
      "Updated the learning rate at epoch NO. 0. New learning rate: 0.01\n",
      "Epoch 1/135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722355221.204585   14279 service.cc:146] XLA service 0x7960e40272b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1722355221.204636   14279 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-07-30 19:30:21.435440: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-07-30 19:30:22.005874: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n",
      "2024-07-30 19:30:23.963267: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.30GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-30 19:30:23.963319: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.30GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-30 19:30:23.963333: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.27GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-30 19:30:24.038719: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.27GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-30 19:30:24.038752: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 9.14GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-30 19:30:24.038767: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 9.14GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-30 19:30:24.038781: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 17.10GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-30 19:30:24.095668: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.30GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-30 19:30:24.095701: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.30GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-30 19:30:24.095715: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.27GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "E0000 00:00:1722355225.322056   14279 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1722355225.464871   14279 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2024-07-30 19:30:29.257165: W external/local_xla/xla/service/hlo_rematerialization.cc:3005] Can't reduce memory use below 1.56GiB (1672484982 bytes) by rematerialization; only reduced to 3.36GiB (3606565672 bytes), down from 3.87GiB (4160077193 bytes) originally\n",
      "2024-07-30 19:30:29.551142: W external/local_xla/xla/service/gpu/nvptx_compiler.cc:762] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.3.107). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  1/982\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:24:49\u001b[0m 16s/step - loss: 18.7119"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 19:30:31.309392: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_12', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "I0000 00:00:1722355231.342839   14279 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 77/982\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 93ms/step - loss: 15.9662"
     ]
    }
   ],
   "source": [
    "# See if the directory to save the checkpoints exists\n",
    "if not os.path.isdir(f\"{os.getcwd()}/model_data\"):\n",
    "    os.mkdir(f\"{os.getcwd()}/model_data\")\n",
    "\n",
    "# Instantiate the checkpoint object\n",
    "chkPoint = ModelCheckpoint(filepath='./model_data/model_{epoch:02d}-{val_loss:.2f}.keras',\n",
    "                                    save_best_only=True,\n",
    "                                    monitor='val_loss',\n",
    "                                    mode='min',\n",
    "                                    verbose=1\n",
    "                            )\n",
    "\n",
    "batch_size = 1\n",
    "LR_schedule = [\n",
    "    (0, 0.01),\n",
    "    (75, 0.001),\n",
    "    (105, 0.0001),\n",
    "]\n",
    "\n",
    "dfTrain = annotationsToDataframe(f\"../data/labels/train\", \"txt\")\n",
    "trainingBatchGenerator = dataGenerator_YOLOv1(f\"../data/images/train\", batch_size, (448,448), dfTrain, 1, True)\n",
    "\n",
    "dfTest = annotationsToDataframe(f\"../data/labels/test\", \"txt\")\n",
    "testingBatchGenerator = dataGenerator_YOLOv1(f\"../data/images/test\", batch_size, (448,448), dfTrain, 1, True)\n",
    "\n",
    "print(f\"Training data: {trainingBatchGenerator.indexes.shape[0]}, testing data: {testingBatchGenerator.indexes.shape[0]}\")\n",
    "\n",
    "model.fit(x=trainingBatchGenerator,\n",
    "        steps_per_epoch = int(trainingBatchGenerator.indexes.shape[0] // batch_size),\n",
    "        epochs = 135,\n",
    "        verbose = 1,\n",
    "        validation_data = testingBatchGenerator,\n",
    "        validation_steps = int(testingBatchGenerator.indexes.shape[0] // batch_size),\n",
    "        callbacks = [\n",
    "            customLearningRate(lrScheduler, LR_schedule),\n",
    "            chkPoint,\n",
    "        ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
